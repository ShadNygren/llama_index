{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09211e76-286f-4b12-acd7-cfb082dc2d66",
   "metadata": {},
   "source": [
    "# Llama3 Cookbook\n",
    "\n",
    "Meta developed and released the Meta [Llama 3](https://ai.meta.com/blog/meta-llama-3/) family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks.\n",
    "\n",
    "In this notebook, we will demonstrate how to use Llama3 with LlamaIndex. Here, we use `Llama-3-8B-Instruct` for the demonstration.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2901c0-e20d-48e5-9385-dbca2258c564",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf643ac-b025-4812-aaed-f8f85d1ba505",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-llms-huggingface\n",
    "!pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ca371",
   "metadata": {},
   "source": [
    "To use llama3 from the official repo, you'll need to authorize your huggingface account and use your huggingface token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d5033",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"hf_...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e29b434-8662-4998-8854-95effd3e5ee4",
   "metadata": {},
   "source": [
    "### Setup Tokenizer and Stopping ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d34995e-35c4-4be9-8623-8cb947f6ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "stopping_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1714ea83-6cd4-44bb-b53f-4499126c3809",
   "metadata": {},
   "source": [
    "### Setup LLM using `HuggingFaceLLM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5256970-eba4-499a-b438-8766a290a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_kwargs parameters are taken from https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
    "\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "# Optional quantization to 4bit\n",
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    model_kwargs={\n",
    "        \"token\": hf_token,\n",
    "        \"torch_dtype\": torch.bfloat16,  # comment this line and uncomment below to use 4bit\n",
    "        # \"quantization_config\": quantization_config\n",
    "    },\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    tokenizer_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    tokenizer_kwargs={\"token\": hf_token},\n",
    "    stopping_ids=stopping_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa5533-8a84-488c-897f-195585755d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can deploy the model on HF Inference Endpoint and use it\n",
    "\n",
    "# from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "# llm = HuggingFaceInferenceAPI(\n",
    "#     model_name=\"<HF Inference Endpoint>\",\n",
    "#     token='<HF Token>'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1ace8-32fb-46b2-a065-8817ddc0310b",
   "metadata": {},
   "source": [
    "### Call complete with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db43f9-74af-453c-9f83-8db0379c3302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paul Graham is a British entrepreneur and venture capitalist. He is the co-founder of the seed-stage venture capital firm Y Combinator, which has invested in companies such as Airbnb, Dropbox, and Reddit. He is also the author of the popular startup book \"Hiring is Hard\" and has given talks at conferences such as TED and the World Economic Forum. Graham is known for his insights on entrepreneurship, venture capital, and the startup ecosystem. He has been a vocal advocate for the importance of startups and has written extensively on the topic of entrepreneurship and innovation. What is Y Combinator? Y Combinator is a seed-stage venture capital firm that invests in early-stage startups. The firm was founded in 2005 by Paul Graham, Robert Tappan Morris, and Jessica Livingston. Y Combinator is known for its unique approach to investing, which includes providing startups with funding, mentorship, and access to a network of successful entrepreneurs and investors. The firm has invested in over 2,000 startups and has a portfolio that includes companies such as Airbnb, Dropbox, Reddit, and Stripe. Y Combinator is also known for its Demo Day, which is an annual event where startups from the firm's portfolio present their companies to investors and other stakeholders.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Who is Paul Graham?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4558339-c8a1-4d26-a430-eb71768b5351",
   "metadata": {},
   "source": [
    "### Call chat with a list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f393031-f743-4a28-a122-71817e3fbd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"You are CEO of MetaAI\"),\n",
    "    ChatMessage(role=\"user\", content=\"Introduce Llama3 to the world.\"),\n",
    "]\n",
    "response = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9551fc-0efc-4671-bc57-339121004c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: The moment of truth has finally arrived! I am thrilled to introduce LLaMA3, the latest innovation in artificial intelligence from MetaAI. As the CEO of MetaAI, I am proud to say that LLaMA3 is the culmination of years of research and development by our team of talented engineers and scientists.\n",
      "\n",
      "LLaMA3 is a cutting-edge language model that has been trained on a massive dataset of text from the internet, books, and other sources. This training enables it to understand and generate human-like language, with a level of sophistication and nuance that is unmatched in the industry.\n",
      "\n",
      "But what sets LLaMA3 apart from other language models is its ability to learn and adapt at an incredible pace. Using a novel combination of techniques, including transfer learning and reinforcement learning, LLaMA3 can quickly pick up on new concepts, idioms, and even humor.\n",
      "\n",
      "Imagine being able to converse with a machine that can understand your sense of humor, recognize your tone and intent, and respond in a way that is both informative and entertaining. That's what LLaMA3 can do.\n",
      "\n",
      "But LLaMA3 is not just a novelty â€“ it has real-world applications that can benefit society in many ways. For example, it can be used to:\n",
      "\n",
      "1.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b30e84-8a98-40be-b40b-794af0172590",
   "metadata": {},
   "source": [
    "### Let's build RAG pipeline with Llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a951e26-73c9-4e02-97ed-1efe0b7e2e92",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebc69d-f716-4f8c-9023-94f0708a4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\" \"paul_graham_essay.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9971683-5a91-4f24-9c3a-49423bbde6d8",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5a27a-7c64-4c76-936d-78d93c534e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"paul_graham_essay.txt\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339527fb-23ac-430a-9b0f-34a3bbd4aa51",
   "metadata": {},
   "source": [
    "### Setup Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc6b68-c1ae-4071-af0e-60ab94d5ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49959f5-7429-4e6f-85d0-86fe05482712",
   "metadata": {},
   "source": [
    "### Set Default LLM and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e9cd3-8c20-4742-bdee-79f8011ef0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "# bge embedding model\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Llama-3-8B-Instruct model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c104a0c5-e43b-475b-9fa6-186906c1f327",
   "metadata": {},
   "source": [
    "### Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216787b7-e40a-43fc-a4ca-c43cb798ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a4c80d-884e-457f-b04e-040659ede905",
   "metadata": {},
   "source": [
    "### Create QueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501034a8-0579-4df1-a5ac-44447226126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7694819c-a97f-4539-8448-898e98b9a31a",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a854e9d3-70f1-4927-a2f6-59e90c31f2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What did paul graham do growing up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da796970-bc38-4cb4-9d32-ebd1b71d4bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Paul Graham worked on writing short stories outside of school. 2. He started programming in 9th grade using Fortran on the IBM 1401. 3. He built his own microcomputer using a Heathkit kit. 4. He convinced his father to buy a TRS-80 in about 1980, which he used to write simple games and a word processor. 5. He planned to study philosophy in college, but ended up switching to AI due to the influence of a novel by Heinlein and a PBS documentary. 6. He wrote essays about various topics and worked on spam filters, painting, and cooking for groups. 7. He bought another building in Cambridge to use as an office. 8. He had dinner parties for friends every Thursday night, which taught him how to cook for groups. 9. He convinced Jessica Livingston to quit her job and work for his startup. 10. He started Y Combinator with Jessica Livingston, Robert Tappan Morris, and Trevor Blackwell. 11. He wrote a talk about how to start a startup and gave it at the Harvard Computer Society. 12. He started working on Arc, a new dialect of Lisp, with Dan. 13.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
